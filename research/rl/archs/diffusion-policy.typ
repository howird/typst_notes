#import "../styles/things.typ": challenge, hypothesis, question

= Diffusion Policy Visuomotor: Policy Learning via Action Diffusion

== Overview

This paper introduces *Diffusion Policy*, a novel method for learning robot
visuomotor policies from demonstrations (imitation learning). It frames the
policy as a conditional denoising diffusion process, which iteratively refines
random noise into a sequence of executable actions conditioned on visual and
proprioceptive observations.

=== Challenges

This work aims to solve several key challenges in robot policy learning:

#challenge[
  Modeling multimodal action distributions
][
  where multiple valid action sequences exist for a given observation (e.g.,
  going left or right of an obstacle).

  #hypothesis[
    By representing the policy as a diffusion model that learns the score
    function ($nabla_a log p(a|o)$) of the action distribution, it can naturally
    capture complex, multi-peaked distributions without mode-averaging or
    mode-collapse.
  ]

  The policy does not directly output an action. Instead, it learns a noise
  prediction network $epsilon_theta(O_t, A_t^k, k)$ that approximates the
  gradient of the log-probability of the action distribution. During inference,
  actions are generated by starting with Gaussian noise and iteratively applying
  stochastic Langevin dynamics (denoising steps) using the learned gradient
  field. Alternative Solutions Discussed: Explicit Policies: Direct regression
  (can't handle multimodality) , Mixture Density Networks (e.g., LSTM-GMM,
  sensitive to hyperparameters) , and action discretization (suffers from
  exponential growth in dimensionality, e.g., BET). Implicit Policies:
  Energy-Based Models (e.g., IBC), which can model multimodality but often
  suffer from training instability.
]

#challenge[
  Handling high-dimensional action spaces
][
  specifically for predicting sequences of future actions to ensure temporal
  consistency and avoid myopic behavior.

  #hypothesis[
    The demonstrated ability of diffusion models to scale to high-dimensional
    outputs (like images) can be leveraged to predict an entire sequence of
    future robot actions jointly.
  ]

  The policy is designed to output a sequence of actions ($A_t$) over a
  prediction horizon ($T_p$). This high-dimensional output is then used in a
  receding horizon control loop, where a subset of the predicted actions ($T_a$)
  is executed before the policy replans. Alternative Solutions Discussed:
  Single-step action prediction, common in methods like BC-RNN, which can lead
  to jittery actions when modes are switched between timesteps.
]

#challenge[
  Training instability
][
  particularly in energy-based models like IBC that require negative sampling.

  #hypothesis[
    A diffusion-based formulation can bypass the need to estimate the
    intractable normalization constant ($Z(o, theta)$) that causes instability
    in other energy-based models.
  ]

  The training loss is a simple Mean Squared Error between the true noise added
  to an action and the noise predicted by the network $epsilon_theta$. This
  objective directly optimizes the score function, which is independent of the
  normalization constant $Z(o, theta)$, leading to stable training. Alternative
  Solutions Discussed: IBC's InfoNCE-style loss, which relies on negative
  samples $tilde(a)^(j)$ to approximate $Z(o, theta)$ and is known to be
  unstable.
]

=== Proposed Component: Diffusion Policy

- *Description*: A visuomotor policy that learns a conditional distribution
  $p(A_t | O_t)$ using a Denoising Diffusion Probabilistic Model (DDPM). It
  generates behavior by treating action generation as a conditional denoising
  process.
- *Inputs*:
  - *Observation Sequence ($O_t$)*: A history of $T_o$ recent observations,
    including sequences of images from one or more cameras and proprioceptive
    robot state (e.g., end-effector pose, gripper status).
  - *Noisy Action ($A_t^k$)*: At each denoising step $k$, the current noisy
    action sequence. The process starts with a pure Gaussian noise vector
    $A_t^K$.
  - *Diffusion Timestep ($k$)*: The current step in the denoising process.
- *Output*:
  - *Action Sequence ($A_t$)*: A denoised sequence of $T_p$ future actions. The
    action space can be either end-effector position (recommended) or velocity.

=== Dependencies

- *Simulation Environments & Benchmarks*:
  - *Robomimic*: A large-scale benchmark for imitation learning, including tasks
    like `Lift`, `Can`, `Square`, `Transport`, and `Tool Hang`.
  - *Push-T*: A simulated planar pushing task adapted from the IBC paper.
  - *Multimodal Block Pushing*: A task adapted from the BET paper to test
    long-horizon multimodality.
  - *Franka Kitchen*: A multi-task, long-horizon environment from the Relay
    Policy Learning paper.
- *Datasets*:
  - Demonstration datasets associated with all the above simulation benchmarks
    (e.g., Robomimic's PH and MH datasets).
  - Human-collected demonstration data for all real-world tasks.
- *Pre-trained Vision Models (used in ablation studies)*:
  - ResNet-18 & ResNet-34 pre-trained on ImageNet-21k.
  - Vision Transformer (ViT-B/16) pre-trained with CLIP.
  - R3M (A Universal Visual Representation for Robot Manipulation).

=== Additional Perspectives

- A key finding is that Diffusion Policy performs significantly better with
  *position control* for the action space, which contrasts with many recent
  behavior cloning works that rely on velocity control. The authors speculate
  this is because Diffusion Policy's strength in modeling multimodality
  overcomes a key drawback of position control, while also benefiting from its
  robustness to compounding errors.
- The paper extensively evaluates the policy on complex, real-world *bimanual
  manipulation* tasks (Egg Beater, Mat Unrolling, Shirt Folding), demonstrating
  its applicability beyond single-arm setups.

=== Assumptions

- The method's success is predicated on the availability of a sufficiently large
  and diverse dataset of *high-quality demonstrations*. As a behavior cloning
  method, it inherits the limitation that it can only imitate behaviors present
  in the training data and struggles with out-of-distribution scenarios or
  suboptimal demonstrations.

=== Recommended Prerequisites

- *Denoising Diffusion Probabilistic Models (DDPMs)*: A solid understanding of
  the core DDPM framework is essential.
  - Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic
    models. *arXiv preprint arXiv:2006.11239*.
- *Implicit Behavioral Cloning (IBC)*: Understanding IBC provides context for
  the challenges (especially training instability) that Diffusion Policy is
  designed to solve.
  - Florence, P., Lynch, C., Zeng, A., et al. (2021). Implicit behavioral
    cloning. *In Conference on Robot Learning*.

== Problem Formulation

The central goal of this paper is to learn a robot's visuomotor policy, denoted
as $pi(A_t | O_t)$, through imitation learning. This policy aims to map a
history of observations $O_t$ to a sequence of future actions $A_t$. The
formulation is designed to overcome common challenges in behavior cloning, such
as handling multimodal action distributions and ensuring stable training.

1. *Conditional Generative Modeling*: The policy is not a direct regression but
  is framed as a conditional generative model based on Denoising Diffusion
  Probabilistic Models (DDPMs). It learns the conditional probability
  distribution $p(A_t|O_t)$.

2. *Action and Observation Spaces*:
  - *Observation ($O_t$)*: A sequence of the last $T_o$ observations. This is a
    multimodal tensor including image sequences from one or more cameras and
    proprioceptive robot states (e.g., end-effector pose, gripper status).
  - *Action ($A_t$)*: A sequence of $T_p$ future actions to be executed by the
    robot, $A_t = a_(t, a_t+1, ..., a_t+T_p-1 )$. This high-dimensional action
    space helps enforce temporal consistency.

3. *The Reverse Process (Learned Policy)*: The core of the policy is the reverse
  diffusion process, which generates a clean action sequence $A_t^0$ by
  iteratively denoising an initial random Gaussian noise vector $A_t^K$. This
  process is governed by a learned *noise prediction network*,
  $epsilon_theta (O_t, A_t^k, k)$, which takes the current noisy action $A_t^k$,
  the observation sequence $O_t$, and the current diffusion timestep $k$ as
  input. The update rule for a single denoising step is:

$
  A_t^(k - 1) = alpha(A_t^k - gamma epsilon.alt_theta (O_t, A_t^k, k) + cal(N)(0, sigma^2 I)) quad(E q . 4)
$

Here, $alpha$, $gamma$, and $sigma$ are hyperparameters from a predefined "noise
schedule" that controls the step sizes. This iterative refinement can be seen as
performing stochastic Langevin dynamics to find a high-likelihood action
sequence.

4. *Score-Based Interpretation*: The noise prediction network $epsilon_theta$ is
  trained to approximate the negative of the score function (the gradient of the
  log-probability of the action distribution).

$
  nabla_(A_t) log p(A_t |O_t) approx - epsilon.alt_theta (O_t, A_t^k, k) quad("Derived from Eq." 8)
$

This is a crucial aspect of the formulation because it allows the model to learn
the shape of the data distribution without needing to compute the intractable
normalization constant $Z(O_t, theta)$ that makes training traditional
Energy-Based Models unstable.

5. *Training Objective*: The network parameters $theta$ are optimized using a
  simple and stable Mean Squared Error (MSE) loss. For a given ground-truth
  action sequence $A_t^0$ from the demonstration data, the model is trained to
  predict the noise $epsilon^k$ that was added to it at a random timestep $k$.

$
  cal(L) = "MSE"(epsilon^k, Epsilon_theta(O_t, A_t^0 + epsilon^k, k)) quad ("Eq." 5)
$

*Note: The paper simplifies the noising term in the equation text; the standard
DDPM noising
$A_t^k = sqrt(overline(alpha)_k) A_t^0 + sqrt(1-overline(alpha)_k)epsilon^k$ is
used in practice.*

== Pipeline

The implementation can be broken down into a training phase, where the model
learns from demonstrations, and an inference phase, where the trained model is
deployed on the robot using receding horizon control.

=== Training

This pipeline describes a single iteration of the training loop.

1. *Data Sampling and Encoding*
  - *Description*: A batch of training instances is sampled from the
    pre-processed demonstration dataset. Each instance consists of a historical
    observation sequence and a corresponding future action sequence. The
    observations are then passed through an encoder.
  - *Input*: A batch of paired data $(O_t, A_t^0)$.
    - `Observation Batch` $O_t$: Tensor of shape $(N, T_o, D_"obs")$, where $N$
      is batch size, $T_o$ is the observation horizon, and $D_"obs"$ represents
      the dimensions of the observation (e.g., $C times H times W$ for images).
    - `Action Batch` $A_t^0$: Tensor of shape $(N, T_p, D_"act")$, where $T_p$
      is the prediction horizon and $D_"act"$ is the action dimension.
  - *Output*: `Observation Embedding` $O_"emb"$: A latent representation of the
    observations, typically with shape $(N, D_"emb")$.

2. *Diffusion Timestep and Noise Generation*
  - *Description*: For each sample in the batch, a random diffusion timestep $k$
    is uniformly sampled. A corresponding random noise vector $epsilon$ is drawn
    from a standard normal distribution.
  - *Input*: Batch size $N$.
  - *Output*:
    - `Timesteps` $k$: Tensor of shape $(N,)$.
    - `Noise` $epsilon$: Tensor of shape $(N, T_p, D_"act")$.

3. *Forward Process (Noising)*
  - *Description*: The ground-truth action sequences $A_t^0$ are corrupted by
    adding the generated noise $epsilon$, scaled according to the sampled
    timesteps $k$ and a predefined noise schedule.
  - *Input*: `Action Batch` $A_t^0$, `Noise` $epsilon$, `Timesteps` $k$.
  - *Output*: `Noisy Actions` $A_t^k$: Tensor of shape $(N, T_p, D_"act")$.

4. *Noise Prediction*
  - *Description*: The core `Diffusion Policy` network ($epsilon_theta$) takes
    the noisy actions, the observation embeddings, and the diffusion timesteps
    as input and predicts the noise that was added. The network can be a CNN or
    a Transformer architecture.
  - *Input*: `Noisy Actions` $A_t^k$, `Observation Embedding` $O_"emb"$,
    `Timesteps` $k$.
  - *Output*: `Predicted Noise` $epsilon_"pred"$: Tensor of shape
    $(N, T_p, D_"act")$.

5. *Loss Calculation and Backpropagation*
  - *Description*: The training loss is calculated as the Mean Squared Error
    between the true noise and the predicted noise, as defined in *Eq. 5*. The
    gradients are then computed and used to update the weights of the noise
    prediction network and the observation encoder.
  - *Input*: `Noise` $epsilon$, `Predicted Noise` $epsilon_"pred"$.
  - *Output*: A scalar loss value, which is then used for backpropagation.

=== Inference (Receding Horizon Control)

This pipeline describes how the trained policy generates actions for the robot
in a closed loop.

1. *Get Current State and Encode*
  - *Description*: The robot's current state, consisting of the last $T_o$
    observations, is captured and passed through the trained observation
    encoder. This encoding is performed only once per planning cycle.
  - *Input*: `Current Observations` $O_"current"$: Tensor of shape
    $(1, T_o, D_"obs")$.
  - *Output*: `Current Observation Embedding` $O_"emb"$: Tensor of shape
    $(1, D_"emb")$.

2. *Initialize Action Sequence*
  - *Description*: The process starts by sampling a random action sequence from
    a standard Gaussian distribution. This will be the initial noisy action at
    the largest timestep, $A_t^K$.
  - *Input*: None.
  - *Output*: `Initial Noisy Action` $A_t^K$: Tensor of shape
    $(1, T_p, D_"act")$.

3. *Iterative Denoising Loop*
  - *Description*: The pipeline enters a loop that iterates from timestep $k=K$
    down to $1$. In each step, it refines the noisy action sequence using the
    learned noise prediction network.
  - *Process*: a. The network $epsilon_theta$ predicts the noise in the current
    action sequence $A_t^k$. b. The predicted noise is used to compute the less
    noisy action for the next step, $A_t^k-1$, by applying the reverse update
    rule from *Eq. 4*. For faster inference, a DDIM-style update is often used,
    which allows for fewer denoising steps than were used in training.
  - *Output of Loop*: A clean, denoised `Action Sequence` $A_t^0$ of shape
    $(1, T_p, D_"act")$.

4. *Execute and Replan*
  - *Description*: The first $T_a$ actions from the generated sequence $A_t^0$
    (where $T_a <= T_p$) are sent to the robot's low-level motor controllers for
    execution. The rest of the sequence is discarded.
  - *Process*: After the $T_a$ actions are executed, the robot captures new
    observations, and the entire inference pipeline (Steps 1-4) is repeated to
    generate the next set of actions. This receding horizon approach ensures the
    robot continuously reacts to changes in its environment.

== Discussion

#question[
  How does Diffusion Policy's general performance compare against
  state-of-the-art imitation learning methods?
][
  Is the Diffusion Policy formulation fundamentally more effective than existing
  methods like Recurrent Neural Networks (BC-RNN/LSTM-GMM), Energy-Based Models
  (IBC), and Transformer-based clustering (BET) across a wide range of standard
  manipulation tasks?
][
  A comprehensive benchmark was conducted on 15 tasks from 4 different
  simulation environments: Robomimic, Push-T, Multimodal Block Pushing, and
  Franka Kitchen. The evaluation included both low-dimensional *state-based*
  observations and high-dimensional *image-based* (vision) observations. Both
  proficient-human (PH) and mixed-quality human (MH) demonstration datasets were
  used to test robustness to data quality.

  *Metrics*: The primary metric was *success rate*, with the exception of the
  Push-T task which used *target area coverage*. Results were reported as both
  the maximum performance achieved across all checkpoints and the average
  performance of the last 10 checkpoints to assess stability.

  *Results*: Diffusion Policy, in both its CNN and Transformer variants,
  significantly and consistently outperformed all baseline methods across nearly
  every task and data modality. The paper reports an average success-rate
  improvement of *46.9%* over the best-performing baseline for each task.
][
  These results provide strong, broad evidence that the diffusion-based policy
  representation is a more powerful and effective foundation for imitation
  learning than prior approaches. The consistent success across diverse tasks
  and observation types validates it as a strong general-purpose method.

  *Limitations*: The authors note that the 46.9% improvement metric is
  calculated relative to the baseline's score, which can inflate the number on
  tasks where baselines perform very poorly.
]

#question[
  Is Diffusion Policy better at handling multimodality in action distributions?
][
  Can the policy effectively model scenarios where multiple distinct action
  sequences are correct (multimodality), a known failure case for simple
  regression-based policies? This was tested for both immediate actions
  (*short-horizon*) and sequences of sub-goals (*long-horizon*).
][
  *Short-Horizon*: A qualitative study on the Push-T task, where the robot can
  push a block from either the left or the right side. The resulting
  trajectories were visualized for different policies.

  *Long-Horizon*: A quantitative study on the *Multimodal Block Pushing* task
  (pushing two blocks to two goals in any order) and the *Franka Kitchen*
  environment (completing a 4-task sequence in any order).

  *Metrics*: For long-horizon tasks, the metric was the frequency of completing
  a certain number of sub-goals (e.g., `p2` for pushing both blocks, `p4` for
  completing all 4 kitchen tasks).

  *Results*: In the Push-T task, Diffusion Policy learned to execute both left
  and right trajectories and would commit to one for a given rollout, whereas
  other methods were biased to one mode or failed to commit (resulting in
  jittery actions). For long-horizon tasks, it dramatically outperformed others,
  achieving a *213%* improvement on the most difficult Kitchen `p4` metric.
][
  This demonstrates one of the core strengths of the diffusion model
  formulation. Its ability to represent a complex, multi-peaked distribution
  allows it to capture the true nature of human demonstrations and solve tasks
  that require choosing between distinct strategies, without averaging them into
  a single, incorrect action.

  *Limitations*: The policy can only model multimodal distributions that are
  present in the demonstration data. It cannot generalize to invent entirely
  new, unseen strategies.
]

#question[
  How does the choice of action space (Position vs. Velocity Control) impact
  Diffusion Policy?
][
  Most recent work in behavior cloning favors velocity control. Does this hold
  true for Diffusion Policy, or does its unique structure allow it to better
  leverage position control?
][
  An ablation study was performed on the "Square" and "Kitchen p4" simulation
  tasks. The performance of Diffusion Policy and baselines (LSTM-GMM, BET) was
  directly compared when using a position-control action space versus a
  velocity-control one.

  *Metric*: The *relative change in success rate* when switching from velocity
  control to position control.

  *Results*: Baseline methods performed worse with position control, consistent
  with prior literature. In stark contrast, Diffusion Policy's performance
  *significantly improved* with position control.
][
  This is a key, counter-intuitive finding. The authors hypothesize that
  Diffusion Policy's inherent strength at modeling multimodality overcomes the
  main drawback of position control, while its action-sequence prediction
  benefits from position control's lower susceptibility to compounding error.
  This provides a strong recommendation for a non-obvious design choice when
  using this method.

  *Limitations*: The finding was demonstrated on a subset of simulation tasks
  and may not hold universally for all possible robotic tasks, especially those
  with very high dynamics where velocity control could be more critical.
]

#question[
  How robust is Diffusion Policy to real-world challenges like system latency?
][
  Real robotic systems have delays (network, computation). How does the policy's
  receding-horizon, action-sequence prediction structure handle this latency?
][
  A simulation-based ablation study was conducted on the Push-T and Square tasks
  where artificial latency was introduced. The delay between when an observation
  was made and when the first corresponding action could be executed was varied
  from 0 to 7 steps.

  *Metric*: The *relative performance change* as latency was increased.

  *Results*: Diffusion Policy demonstrated high robustness to latency,
  maintaining its peak performance with up to 4 steps of delay before a gradual
  decline. This was particularly true when using position control.
][
  This shows the practical viability of the approach. By predicting a future
  sequence of actions, the policy effectively "plans ahead," making it resilient
  to the unavoidable delays in a physical robot control loop.
]

#question[
  What are the limitations of Diffusion Policy?
][
  What are the inherent weaknesses of the proposed method and what are promising
  directions for future work?
][
  This section is a discussion based on the authors' experience and the
  theoretical underpinnings of the model, rather than a direct experiment.

  *Identified Limitations*:
  1. *Data Dependency*: As a form of behavior cloning, Diffusion Policy is
    fundamentally limited by the provided demonstrations. It struggles with
    suboptimal data and cannot generalize far beyond the demonstrated behaviors.
  2. *Computational Cost*: The iterative nature of the denoising process makes
    inference slower and more computationally expensive than single-pass
    explicit policies. This may be a bottleneck for tasks requiring very
    high-frequency control (>10-20 Hz).
][
  This provides a transparent and critical view of the method's boundaries. It
  highlights that while powerful, Diffusion Policy is not a "silver bullet" and
  frames the context for future research. The authors suggest that integrating
  it with reinforcement learning could address the data dependency, and
  leveraging new techniques for accelerating diffusion models could solve the
  latency issue.
]
