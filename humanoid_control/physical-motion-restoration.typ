= A Plug-and-Play Physical Motion Restoration Approach

== Overview

This paper introduces a method to correct physically implausible 3D human
motions extracted from monocular videos, particularly focusing on
high-difficulty and "in-the-wild" movements like gymnastics and martial arts.
The core idea is to create a plug-and-play solution that refines the output of
existing video motion capture algorithms.

=== Challenges and Solutions

- *Challenge 1: Flawed Reference Motions*
  - Video motion capture algorithms often produce short, flawed motion clips
    (e.g., incoherent frames) when dealing with rapid or extreme poses, which
    cause failures in physics-based simulation.
  - *Hypothesis*: While motion capture fails on blurred frames, segmentation
    models are more robust at identifying the general human silhouette. This
    robust signal, combined with the surrounding correct motion context, can be
    used to repair the flawed segments.
  - *Proposed Approach*: A *Mask-conditioned Motion Correction Module (MCM)* is
    proposed. It first detects mismatched frames using keypoint similarity or
    mask overlap. Then, it uses a conditional diffusion model, guided by video
    segmentation masks and motion context, to regenerate and replace the flawed
    motion segment.
  - *Alternative Solutions*: Other methods attempt to avoid these flaws by
    integrating simplified dynamic equations or differentiable physics directly
    into the motion capture process, but they struggle to generalize.

- *Challenge 2: Inherent Imitation Complexity*
  - High-difficulty motions are rare (long-tail distribution) and involve
    complex force control, making it hard for a single, pre-trained motion
    imitation policy to track them accurately without suffering from
    catastrophic forgetting.
  - *Hypothesis*: A general motion prior can be learned through pre-training,
    and then this knowledge can be rapidly adapted to the specific dynamics of a
    single, challenging motion at test time. This avoids the need for a single
    controller to master all possible motions.
  - *Proposed Approach*: A *Physics-based Motion Transfer Module (PTM)* is
    proposed, which uses a "pretrain and adapt" pattern. A controller is first
    pre-trained on a large corpus of diverse motions. Then, during inference, a
    Reinforcement Learning-based Test Time Adaptation (RL-TTA) strategy is used
    to update the network parameters to successfully imitate the specific,
    challenging input motion. This adaptation includes specialized reward
    functions, termination conditions, and the use of residual forces to handle
    complex dynamics.
  - *Alternative Solutions*: Traditional motion imitation methods train a single
    policy to simulate an entire dataset, but these often fail on
    out-of-distribution, high-difficulty motions and are sensitive to the
    quality of the reference motion.

=== Proposed Component: High-Level Description

- *Algorithm*: A two-stage, plug-and-play motion restoration pipeline.
  - The *Mask-conditioned Motion Correction Module (MCM)* first identifies and
    repairs flawed segments in the input motion.
  - The corrected motion is then passed to the *Physics-based Motion Transfer
    Module (PTM)*, which uses a pre-trained policy and test-time adaptation to
    generate a physically plausible version of the motion in a physics
    simulation.
- *Inputs*:
  - A monocular video of a single person.
  - The corresponding 3D motion sequence generated by a video motion capture
    algorithm (referred to as the "reference motion").
- *Output*:
  - A physically restored 3D human motion sequence that is free from artifacts
    like ground penetration, floating, and foot-sliding, while preserving the
    original motion's style and trajectory.

=== Non-Novel Dependencies

- *Datasets*:
  - *Pre-training*: AMASS, Human3.6M, AIST++, and the Motion-X kungfu subset.
  - *Evaluation*: AIST++, EMDB, Motion-X kungfu subset, and a newly collected
    in-the-wild dataset.
- *Pre-trained Models*:
  - A large visual semantic segmentation model (Segment Anything Model - SAM) to
    generate human masks.
  - A Vision Transformer (ViT) as a feature extractor for the segmentation
    masks.
  - An object detection algorithm for 2D keypoint extraction (used in an
    alternative mismatch detection method).
  - The motion diffusion model architecture is based on GMD with a UNet.
- *Environments*:
  - A simulated physical environment for reinforcement learning and motion
    imitation.

=== Assumptions

- The method is designed exclusively for *single-person motions* and does not
  handle multi-person interactions.
- The approach assumes that any flawed motion segments are *brief* and
  surrounded by sufficient correct motion context to enable successful
  correction.
- The method is not designed to handle *human-object interactions*, as these
  were explicitly removed from the evaluation datasets.

== Problem Formulation

The overall problem is to restore the physical plausibility of a 3D human motion
sequence, $x$, captured from a video, while preserving the original motion's
pattern. This is broken down into two sub-problems: correcting flawed motion
segments and then imitating the corrected motion in a physics-based environment.

=== Motion Imitation as a Markov Decision Process (MDP)

The core of the restoration is controlling a simulated humanoid to follow a
reference motion, which is formulated as an MDP defined by the tuple
$M = angle.l S, A, P_"physics", R, gamma angle.r$.

- *State ($s_t$)*: The state at time $t$ consists of the humanoid's joint
  positions, rotations, linear and angular velocities, as well as information
  about the goal state, $g_t$, which is the difference between the current and
  next frame of the reference motion.
- *Action ($a_t$)*: The action is the set of target joint angles for the
  humanoid's internal PD controller.
- *Transition Dynamics ($P_"physics"$)*: The physics simulator determines the
  next state $s_t+1$ after applying torques computed from the action $a_t$. The
  torque for each joint's degree of freedom (DoF) $i$ is calculated using a PD
  controller:

$
  tau^i = k_p^i (a_t^i - x_t^i) - k_d^i q_t^i quad "(Eq. 1)"
$

where $x_t^i$ is the current joint angle, $q_t^i$ is the current joint velocity,
and $k_p, k_d$ are controller gains.
- *Reward ($r_t$)*: The reward function guides the policy. During pre-training,
  it is a sum of a reconstruction reward $r_t^g$, a style reward $r_t^"amp"$
  from an AMP discriminator, and an energy penalty $r_t^"energy"$.

$
  r_t = r_t^g + r_t^"amp" + r_t^"energy" quad "(Eq. 2)"
$

During the Test-Time Adaptation phase for difficult motions, a *relative reward*
is used, which neglects the absolute root position to better handle floating and
jitter in the reference motion. It is formulated as:

$
  r_t^g = e^(w_p norm("rela"(tilde(p)_t) - "rela"(p_t))) + e^(w_r norm(tilde(theta)_t plus.circle theta_t)) + e^(-norm(tilde(v)_t - v_t)) + e^(-norm(tilde(omega)_t - omega_t)) quad "(Eq. 3)"
$

where $tilde(p), tilde(theta)$ are reference positions and rotations, $p, theta$
are the humanoid's positions and rotations, and $"rela"()$ ignores the root
joint's gravity axis component.
- *Termination Condition ($cal(F)_t$)*: To improve efficiency during adaptation,
  a relative termination condition is used. The episode terminates if the mean
  relative distance between the humanoid and reference joints exceeds a
  threshold $d_"term"$, or if a fall or erroneous ground contact is detected.

$
  cal(F)_t = cases(
    1 & "if "norm("rela"(p_t) - "rela"(tilde(p)_t)) > d_"term" or cal(F)_t^h or cal(F)_t^c, 0 & "otherwise",
  ) quad "(Eq. 4)"
$

=== Flawed Motion Correction as Conditional Diffusion

The problem of correcting flawed motion segments is formulated as a conditional
in-betweening task using a diffusion model.

- *Mismatch Detection*: Flaws are first identified by calculating the Object
  Keypoint Similarity (OKS) between the projected 3D motion's keypoints and 2D
  keypoints detected in the video. The OKS score is given by:

$
  "OKS" = (sum_i e x p(-d_i^2 /2 s_i^2 k_i^2) delta(v_i > 0))/(sum_i delta(v_i > 0)) quad "(Eq. 5)"
$

where $d_i$ is the distance between keypoints, $s$ is the object scale, $k_i$ is
a per-keypoint constant, and $v_i$ is a visibility flag. (Note: The paper uses a
simplified notation ). An alternative detection method uses the Intersection
over Union (IoU) between the projected 3D mesh and a 2D segmentation mask.
- *Diffusion Process*: A standard forward diffusion process progressively adds
  noise to the motion data $x_0$ over $N$ steps:

$
  q(x_(1 : N)|x_0) : = product_(n = 1)^N q(x_n |x_(n - 1)) quad "(Eq. 6)"
$

The reverse process uses a learned network, conditioned on the human
segmentation mask and motion context, to denoise the data and reconstruct the
flawed motion segments.

== Pipeline

The project is implemented as a sequential pipeline that takes a video and its
corresponding raw 3D motion as input and outputs a physically corrected version
of that motion.

=== Initial Data Acquisition
- *Description*: A standard video motion capture (VMC) method is used to get an
  initial 3D motion estimate from an input video. Concurrently, a segmentation
  model processes the video to extract human masks.
- *Input*:
  - `video`: A sequence of $N$ RGB frames.
- *Processing*:
  - Run a VMC algorithm (e.g., GVHMR, TRAM) on the `video`.
  - Run a segmentation model (SAM) on the `video`.
- *Output*:
  - `reference_motion` (Tensor Shape: $N times D$): The raw 3D motion sequence,
    where $N$ is the number of frames and $D$ is the dimension of the pose
    representation. The representation includes joint positions
    $p_t in RR^( J times 3 )$ and rotations $theta_t in RR^( J times 6 )$, where
    $J$ is the number of joints.
  - `human_masks` (Tensor Shape: $N times W times H$): A sequence of binary
    segmentation masks for the human subject, where $W$ and $H$ are the video's
    width and height.

=== Flaw Detection (MCM)
- *Description*: This stage identifies frames where the `reference_motion` is
  physically or kinematically incorrect by comparing it against 2D evidence from
  the video.
- *Input*:
  - `reference_motion` ($N times D$)
  - `human_masks` ($N times W times H$)
  - (Alternatively) 2D keypoints extracted from the video.
- *Processing*:
  - For each frame, the 3D `reference_motion` mesh is projected onto the 2D
    image plane.
  - The similarity between the projected mesh and the `human_mask` is calculated
    (e.g., IoU).
  - Alternatively, the similarity between projected 3D keypoints and detected 2D
    keypoints is calculated using *Eq. 5 (OKS)*.
  - Frames with a similarity score below a threshold are flagged as flawed.
- *Output*:
  - `flaw_signal` (Tensor Shape: $N$): A boolean tensor where `True` indicates a
    flawed frame that needs correction.

=== Motion Correction (MCM)
- *Description*: If any flaws were detected in Stage 2, this stage uses a
  conditional diffusion model to repair the flawed segments of the motion. If no
  flaws are detected, this stage is skipped.
- *Input*:
  - `reference_motion` ($N times D$)
  - `human_masks` ($N times W times H$)
  - `flaw_signal` ($N$)
- *Processing*:
  - The `human_masks` are passed through a pre-trained ViT to extract feature
    embeddings.
  - A motion diffusion model (based on GMD) performs conditional in-betweening.
    It takes the noisy motion, the `flaw_signal` (as a keyframe signal), and the
    mask features as input.
  - The model denoises and reconstructs only the motion segments marked by
    `flaw_signal`, guided by the surrounding motion context and the mask
    features, following the reverse process of *Eq. 6*.
- *Output*:
  - `corrected_motion` (Tensor Shape: $N times D$): The motion sequence with
    flawed segments replaced. If no flaws were detected, this is identical to
    `reference_motion`.

=== Physics-Based Imitation (PTM)
- *Description*: The corrected motion is now used as a target reference for a
  pre-trained agent in a physics simulator. The agent attempts to imitate this
  motion.
- *Input*:
  - `corrected_motion` ($N times D$)
- *Processing*:
  - The pre-trained policy $pi_"PTM"$ is loaded.
  - The simulation runs, with the policy receiving state $s_t$ and producing
    actions $a_t$ at each step.
  - Torques are computed via *Eq. 1* and applied to the humanoid.
  - The simulation proceeds. If it completes the entire motion without
    triggering a termination condition (*Eq. 4* with stricter, non-adapted
    thresholds), the process is a success.
- *Output*:
  - `simulated_motion` (Tensor Shape: $N times D$): The physically plausible
    motion generated by the simulation.
  - `success_flag` (Boolean): A flag indicating if the imitation was successful.

=== Test-Time Adaptation (PTM-TTA)
- *Description*: This stage is activated only if Stage 4 fails (`success_flag`
  is `False`), which is common for high-difficulty motions. The policy network
  is fine-tuned specifically for the current difficult motion.
- *Input*:
  - `corrected_motion` ($N times D$)
  - The failed state from the PTM controller.
- *Processing*:
  - The RL-TTA strategy is initiated.
  - The policy network parameters are updated for a limited number of steps
    (e.g., 2,000-4,000) using PPO.
  - This adaptation uses the specialized *relative reward* function (*Eq. 3*)
    and the more lenient *relative termination* condition (*Eq. 4*) to
    facilitate learning on noisy, complex motions.
  - Residual forces may be added to compensate for dynamics mismatches, such as
    mimicking the effect of a trampoline.
  - The process is repeated until the motion is successfully imitated or a step
    threshold is reached.
- *Output*:
  - `restored_motion` (Tensor Shape: $N times D$): The final, physically
    plausible motion sequence that successfully tracks the high-difficulty
    reference.

== Discussion

Here is a detailed outline of the main questions the paper aims to answer, the
experiments designed to address them, and the corresponding results and
limitations.

=== Overall Performance vs. State-of-the-Art
- *Question*: How does the proposed plug-and-play method compare against
  state-of-the-art (SOTA) motion capture and physics-informed methods in terms
  of physical realism, motion accuracy, and consistency with the source video?
- *Experiments*: The authors compared their method against baseline SOTA video
  motion capture methods (TRAM, GVHMR) and a SOTA physics-informed method
  (PhysPT). The proposed method was applied as a post-processing step to the
  outputs of TRAM and GVHMR. Evaluations were conducted on the AIST++, Kungfu,
  EMDB, and a custom in-the-wild dataset.
- *Metrics Used*:
  - *Physical Authenticity*: Self-Penetration (SP), Ground-Penetration (GP),
    Float, and Foot-Skate (FS).
  - *World & Camera Coordinate Accuracy*: WA-MPJPE (World-aligned Mean Per Joint
    Position Error), W-MPJPE, MPJPE, and PA-MPJPE (Procrustes-aligned MPJPE).
  - *2D Similarity*: OKS (Object Keypoint Similarity) and MPS (Mask-Pose
    Similarity) to measure how well the final 3D motion matches the original
    video.
- *Results and Significance*:
  - The method demonstrated *significant improvements in all physical
    authenticity metrics* across every dataset. For example, on the EMDB
    dataset, it reduced ground penetration from 82 to 0.24, proving its core
    ability to fix physical artifacts.
  - It generally *improved or maintained world-coordinate accuracy*, as the
    physics simulation helps mitigate the accumulation of tracking errors over
    long sequences.
  - Results in camera coordinates were mixed, with slight degradations on some
    datasets. This is because the method optimizes for physical plausibility in
    a simulated world, not for perfect alignment with a specific 2D camera view.
  - The results validate the approach as an effective "plug-and-play" module
    that enhances physical realism without catastrophically degrading the
    original motion's accuracy.
- *Limitations*: The focus on physical space over camera space means the method
  may not be ideal for applications where perfect 2D reprojection fidelity is
  the primary goal.

=== Capability on High-Difficulty In-the-Wild Motions
- *Question*: Can the proposed method restore complex motions from in-the-wild
  videos where other advanced physics-based (PhysPT) and motion imitation (PHC+)
  methods fail?
- *Experiments*: A qualitative visual comparison was conducted on a challenging
  gymnastics motion captured by GVHMR. The outputs of PhysPT, PHC+, the proposed
  method without the motion correction module (Ours w/o MCM), and the full
  proposed method were visualized and compared.
- *Metrics Used*: The evaluation was *qualitative*, based on visual inspection
  for physical artifacts (floating, penetration), motion coherence, and
  simulation failure (e.g., falling).
- *Results and Significance*:
  - Both SOTA methods, PhysPT and PHC+, *failed to track the complex, noisy
    motion*.
  - The proposed method without MCM also *failed*, but specifically at the point
    where the input motion was flawed, highlighting the necessity of the
    correction module.
  - The *full method successfully restored the entire motion*, correcting
    physical artifacts and overcoming the flawed input segment that caused other
    methods to fail.
  - This result is significant as it demonstrates the method's unique robustness
    and its ability to handle a class of challenging motions that are beyond the
    reach of current alternatives.
- *Limitations*: The evidence is compelling but qualitative and based on a
  selected example. It doesn't provide a quantitative success rate across a
  large set of similarly difficult motions.

=== Effectiveness of the Physics-based Motion Transfer (PTM) Module
- *Question*: Is the core "pretrain and adapt" strategy of the PTM module
  superior at tracking difficult motions compared to other SOTA motion imitation
  frameworks?
- *Experiments*: The PTM module was isolated and evaluated on its ability to
  track motions from the difficult Kungfu dataset. Its performance was compared
  against two leading motion imitation methods, UHC and PHC+.
- *Metrics Used*:
  - *Success Rate (SR)*: The percentage of motion sequences successfully
    imitated without failure.
  - *MPJPE*: To measure the accuracy of the motion tracking.
- *Results and Significance*:
  - The PTM module achieved a *98.16% success rate*, dramatically outperforming
    both UHC (42.91%) and PHC+ (76.41%).
  - This quantitatively proves the superiority of the test-time adaptation (TTA)
    strategy for high-difficulty motions. By fine-tuning the policy for each
    specific motion, PTM avoids the "catastrophic forgetting" problem that
    limits single, fixed policies when dealing with a wide and complex range of
    movements.
- *Limitations*: This ablation focuses on a specific type of high-difficulty
  motion (Kungfu). The performance gap might be less pronounced on datasets
  comprised of more common, everyday motions.

=== Impact of RL-TTA Strategy Components
- *Question*: Which specific designs within the RL-based Test-Time Adaptation
  (RL-TTA) strategy contribute most to its success?
- *Experiments*: An ablation study was conducted on the in-the-wild dataset
  where components of the RL-TTA were incrementally added: a baseline
  pre-trained policy, a relative early termination condition (Early-Term),
  residual forces (Res-F), a relative reward function (Rela-Rwd), and the full
  Test-Time Adaptation.
- *Metrics Used*: Success Rate (SR), OKS, and MPS were used to measure
  performance.
- *Results and Significance*:
  - The baseline pre-trained policy performed poorly on its own, with only a 37%
    success rate.
  - Each component provided an incremental benefit, but the *Test-Time
    Adaptation itself provided the largest performance leap*, increasing the
    success rate from 61% to 85%.
  - This pinpoints the adaptation process, rather than just specialized rewards
    or forces, as the most critical element for handling difficult motions. It
    confirms that dynamically optimizing the policy for a specific challenge is
    highly effective.
- *Limitations*: The study was performed on a high-difficulty dataset. The
  relative importance of each component might be different for simpler motions
  where adaptation may not even be necessary.

=== 5. Effectiveness of the Mask-conditioned Motion Correction (MCM) Module
- *Question*: How critical is the MCM for achieving a high success rate, and
  what is the best signal (masks vs. keypoints) to guide the motion correction?
- *Experiments*: An ablation study was performed on the in-the-wild dataset to
  analyze the MCM. The study compared performance with and without the
  correction module and tested different conditioning signals (human masks vs.
  2D keypoints) for both mismatch detection and the diffusion model's guidance.
- *Metrics Used*: Success Rate (SR) and Mask-Pose Similarity (MPS).
- *Results and Significance*:
  - Employing the MCM to correct bad frames *significantly improved the
    simulation success rate* (e.g., from 78% to 87%), demonstrating that
    repairing flawed motions before simulation is critical for avoiding
    failures.
  - Using *human masks as the conditioning signal consistently outperformed 2D
    keypoints*. The paper suggests this is because masks provide more robust and
    comprehensive shape information, whereas keypoint detection can be
    unreliable during complex and rapid movements.
- *Limitations*: The study does not quantify how frequently such "flawed
  motions" appear in standard datasets versus the more challenging in-the-wild
  set, making it hard to gauge the MCM's impact on less difficult data.

=== Overall Method Limitations
- *Question*: What are the primary scope limitations and unresolved challenges
  of the proposed work?
- *Experiments*: This was not addressed by an experiment but was explicitly
  stated by the authors in the conclusion.
- *Results and Significance*:
  - The method is currently limited to *single-person scenarios* and cannot
    handle multi-person interactions.
  - The *inference time can be high* for difficult motions that require
    thousands of TTA steps, making it unsuitable for real-time applications.
  - This clearly defines the boundaries of the current research and provides a
    clear path for future work, which could focus on multi-person physics,
    human-object interaction, and computational optimization.
